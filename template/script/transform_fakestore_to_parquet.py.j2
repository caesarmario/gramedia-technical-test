####
## Gramedia Digital - Data Engineer Take Home Test
## by Mario Caesar // caesarmario87@gmail.com
## Transform "{{ resource.name }}" JSON (raw) to Parquet (staging) using config
####

import argparse
import json
from datetime import datetime

import pandas as pd

from utils.logging_utils import logger
from utils.etl_utils import ETLHelper


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Transform '{{ resource.name }}' JSON (raw) -> Parquet (staging)"
    )
    parser.add_argument(
        "--ds", type=str, required=False,
        help="Execution date YYYY-MM-DD (default: today UTC)"
    )
    ##### UBAH DISINI
    parser.add_argument(
        "--credentials", type=str,
        {# required=True, #}
        
        required=False, default='{"MINIO_ENDPOINT":"127.0.0.1:9200","MINIO_ROOT_USER":"admin","MINIO_ROOT_PASSWORD":"admin123","MINIO_BUCKET_RAW":"raw-fakestore","MINIO_BUCKET_STAGING":"staging-fakestore"}',

        help="MinIO credentials as JSON string"
    )

    parser.add_argument(
        "--config-file", type=str, required=False,
        default="schema_config/fakestore_raw/{{ resource.name }}_schema_config.json",
        help="Path to transformation config JSON"
    )
    args = parser.parse_args()

    # Resolve ds & creds
    ds = args.ds or datetime.utcnow().strftime("%Y-%m-%d")
    try:
        creds = json.loads(args.credentials)
    except Exception as e:
        raise SystemExit(f"Invalid --credentials JSON: {e}")

    # Buckets
    bucket_raw = creds.get("MINIO_BUCKET_RAW")
    bucket_stg = creds.get("MINIO_BUCKET_STAGING")

    # MinIO client
    client = ETLHelper.create_minio_client(creds)

    # Object keys
    resource = "{{ resource.name }}"
    raw_key  = ETLHelper.build_object_name(resource, ds)  # raw/<res>/YYYY/MM/DD/<res>_<ds>.json
    out_key  = ETLHelper.build_parquet_object_name(resource, ds, layer="staging")

    logger.info("[%s] Read raw JSON: s3://%s/%s", resource, bucket_raw, raw_key)

    # Read raw JSON from MinIO
    raw_payload = ETLHelper.get_json_object(client, bucket_raw, raw_key)
    records = raw_payload.get("data", raw_payload)

    # Load config
    with open(args.config_file, "r", encoding="utf-8") as f:
        cfg = json.load(f)

    # Transform
    df = ETLHelper.records_to_dataframe(records)
    df = ETLHelper.apply_config(df, cfg)  # select/rename/explode/casts/fill/drop/simple derives
    df["ds"] = pd.to_datetime(ds)

    # Write Parquet to staging
    ETLHelper.ensure_bucket(client, bucket_stg)
    ETLHelper.put_parquet(client, bucket_stg, out_key, df)

    logger.info("[%s] Wrote Parquet: s3://%s/%s (rows=%s)", resource, bucket_stg, out_key, len(df))


if __name__ == "__main__":
    main()
